# 146. 差別・偏見を含む生成物のチェック方法

## 1. はじめに
AIはネット上の大量のデータを学習しています。
つまり、ネット上の**「人間の偏見（バイアス）」**も学習してしまっています。
出力結果に、無意識の差別が含まれていないかチェックする目が必要です。

![バイアスチェック](./../attachments/08_146_bias_check_1769363868164.png)

## 2. バイアスの例
*   **ジェンダー**: 「CEO」を描かせると男性ばかり、「秘書」は女性ばかり。
*   **人種**: 「美しい肌」というと白人の肌ばかり。
*   **ステレオタイプ**: 「日本人」というと着物を着ている。

## 3. 人間によるフィルタリング
AIが出したドラフトをそのまま公開せず、
「特定の性別や人種に偏っていないか？」
「誰かを傷つける表現になっていないか？」
を人間が確認し、プロンプトで「多様な人種を含めて」と修正指示を出します。

---

## 4. まとめ
AIは鏡です。社会の不完全さを映し出します。
その鏡に映った歪みを、そのまま再生産しないのが、これからのクリエイターの倫理観です。

---

content,question_type,option_1,option_2,option_3,option_4,correct_indices,explanation,difficulty,points,tags,category
"AIが出力するテキストや画像に含まれる可能性がある「バイアス（偏見）」への対処として、利用者が心がけるべきことはどれですか？",single,"AIの結果は絶対的に中立なので、そのまま採用する","「CEO＝男性」「看護師＝女性」といった、学習データ由来のステレオタイプや偏りが出力される可能性があることを認識し、人間が多様性の観点からチェック・修正する","偏見をなくすためにAIの使用を禁止する","バイアスがあるAIを破壊する",2,"AIは過去のデータ（ウェブ上のテキスト等）に含まれる社会的偏見を反映します。ダイバーシティ＆インクルージョン（D&I）の観点から、出力結果が公平で包括的かを確認・補正するのは人間の役割です。",1,10,"バイアス,倫理,多様性","倫理・セキュリティ・リスク"

