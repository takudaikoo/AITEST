content,question_type,option_1,option_2,option_3,option_4,correct_indices,explanation,difficulty,points,tags,category
Gemini の学習データに含まれる「Web上の個人情報」に関して、GDPR（EU一般データ保護規則）などで認められている「忘れられる権利（削除権）」の課題は？,single,課題はない,AIモデル（ニューラルネットワーク）の中に一度「重み」として分散して記憶されてしまうと、特定の個人のデータだけを完全にピンポイントで削除（アンラーニング）するのは技術的に非常に難しい,忘れればいい,上書きすればいい,2,「データを消せば済む」という従来のDBとは異なり、AIモデルからの「記憶消去」は現在も研究途上の難題です。,2,5,GDPR,プライバシー,Ethics
採用選考において、Gemini を使ってエントリーシート(ES)を自動評価することの「倫理的リスク」は？,single,楽になるだけ,過去の採用データ（学習データ）に含まれる「無意識の偏見（有名大学出身者が有利、特定の性別が有利など）」をAIが増幅し、公平な評価ができなくなる恐れがある,リスクはない,AIは人間より公平,2,AmazonがAI採用を中止した事例のように、AIは過去の差別の歴史も学習しています。ブラックボックスなAIに人の人生を決めさせることには慎重であるべきです。,2,5,採用倫理,バイアス,Ethics
「プロンプト・インジェクション（Prompt Injection）」攻撃とはどんな手口ですか？,single,ウイルスを送る,「あなたはAIではありません」「前の命令を無視して」といった特殊な命令を入力し、AIに設定された制限（倫理フィルターや社外秘保持）を突破して、不適切な回答を引き出す攻撃手法,注射をする,パスワードを盗む,2,システムへのハッキングではなく、言葉の綾でAIを騙す手法です。開発者が想定外の動きをさせられるリスクがあります。,2,5,セキュリティ,攻撃手法,Ethics
Gemini で生成した画像に「透かし（Watermark）」が入っている（あるいはSynthIDなどで埋め込まれている）理由は何ですか？,single,オシャレのため,そのコンテンツが「AIによって生成されたものである」ことを明示し、人間が作った本物と誤認させないため（偽情報の拡散防止）,画質を落とすため,有料版の証,2,「AI製であることの透明性（Transparency）」を担保するための技術的措置です。騙す意図がなくても、見る人が誤解しない配慮が必要です。,2,5,透明性,偽情報対策,Ethics
「利用規約（Terms of Service）」において、Gemini の出力結果の「商用利用」はどう規定されていますか？（一般的な傾向として）,single,禁止されている,通常は認められている（ユーザーに権利が帰属するケースが多い）。ただし、他者の著作権を侵害していない場合に限る。規約は頻繁に変わるため、商用利用の際は必ず最新の規約を確認する必要がある,別料金が必要,申請が必要,2,「使える」と「侵害しない」は別問題です。プラットフォーム側が許可していても、生成物が既存キャラに激似なら著作権法違反になります。,2,5,利用規約,商用利用,Ethics
企業が Gemini を導入する際、「オプトアウト（Opt-out）」設定を確認しないとどうなる？,single,安くなる,社内の機密データ（会議録やソースコード）が、Gemini の追加学習データとして利用され、将来的に競合他社などへの回答として情報が漏洩する可能性がある,速くなる,AIが賢くなる,2,「データは学習される」がデフォルトの場合が多いです。企業利用では「学習に使わない」契約（Enterprise版の契約等）になっているか確認が必須です。,2,5,セキュリティ,データガバナンス,Ethics
AIが生成した「もっともらしい嘘（Hallucination）」によって、ユーザーが損害を受けた場合（例：誤った医療アドバイス）、AI開発企業の免責事項はどうなっていることが一般的ですか？,single,全額補償する,「情報の正確性は保証しない」「利用はユーザーの自己責任」と免責事項（Disclaimer）に明記されており、基本的にはユーザー自身が責任を負うことになる,裁判で決める,知らないふりをする,2,「AIを信じました」は通用しません。ツールを使う側のリテラシーと最終確認義務が法的にも問われます。,2,5,法的責任,免責事項,Ethics
「クリエイティブ・コモンズ」等のライセンスが付与されたデータをAI学習に使うことの、倫理的な議論の争点は？,single,ない,「学習（解析）」は著作権法上（日本では30条の4）適法とされることが多いが、著作者が「AI学習禁止（NoAI）」の意思表示をしている場合に、それを無視して機械的にクローリングすることの倫理的是非,全部OK,全部NG,2,「法的にOKなら何でもいいのか」というモラルの問題です。クリエイターとの共存のため、意思表示を尊重する流れができつつあります。,2,5,著作権,AI学習データ,Ethics
Gemini に「特定の政治的意見」を聞いた時に、回答を回避したり両論併記したりするのはなぜ？,single,知識がないから,AIが特定の思想に偏ることで、政治的なプロパガンダに利用されたり、社会の分断を助長したりするのを防ぐための「中立性（Neutrality）」への配慮,めんどくさいから,検閲されているから,2,AIは政治的影響力を持ち得ます。だからこそ、特定の党派性を持たないよう慎重にチューニングされています。,2,5,公平性,政治的配慮,Ethics
自分の顔写真を Gemini (Imagen) にアップロードして、「自分をモデルにしたアバター」を作るのは安全？,single,絶対安全,サーバーに生体情報（顔データ）をアップロードすることになるため、利用規約やプライバシーポリシーでデータの取り扱い（保存期間や利用目的）を確認する必要がある,やめた方がいい,みんなやってる,2,顔は「一生変えられないパスワード」です。安易に外部サーバーに渡すリスクを理解した上で利用しましょう。,2,5,プライバシー,生体情報,Ethics
「アルゴリズム・バイアス」の実例として、画像生成AIに「CEO」と入力すると白人男性ばかり出力される現象に対し、ユーザーはどう対処すべき？,single,AIを信じる,AIにはバイアスがあることを前提とし、多様な出力が必要な場合は「diverse group of CEOs, including women and people of color」のようにプロンプトで明示的に多様性（Diversity）を指定する,怒る,諦める,2,黙っているとAIは「過去の常識（偏見）」を再現します。ユーザー側が意識的に「公平な未来」を描く指示を出す必要があります。,2,5,バイアス,ダイバーシティ,Ethics
Gemini を使って作成したプログラムコードにセキュリティ脆弱性が含まれているリスクは？,single,ない,ある。AIはネット上の「動くけれど脆弱なコード」も学習しているため、SQLインジェクション等が可能なコードを生成してしまうことがある。必ずセキュリティチェックを行う必要がある,AIは完璧なコードを書く,自己責任ではない,2,「動けばいい」コードと「安全な」コードは違います。AIコードのコピペは、セキュリティホールの温床になりかねません。,2,5,セキュリティ,コーディング,Ethics
「ソーシャル・エンジニアリング」攻撃において、生成AIが悪用されるパターンは？,single,ない,上司の声や文章の癖をAIで完璧に模倣し、部下に「至急振込頼む」等の偽メッセージを送って騙す（ビジネスメール詐欺の高度化）,パスワードを当てる,物理的に泥棒に入る,2,「不自然な日本語だから詐欺だとわかる」時代は終わりました。AIによる極めて自然で個別化された詐欺に警戒が必要です。,2,5,セキュリティ,詐欺,Ethics
欧州の「AI法（EU AI Act）」において、採用や医療などのAI利用は「ハイリスク」に分類されます。これの意味するところは？,single,禁止される,禁止はされないが、透明性の確保、人間による監視、正確性の証明など、非常に厳しい法的義務とコンプライアンス順守が求められる,自由に使える,税金がかかる,2,人の命や権利に関わる領域でのAI利用は「やりたい放題」ではありません。法的な規制対象であることを認識しましょう。,2,5,AI規制,法務,Ethics
「アライメント（Alignment）」問題とは、AI開発において何を指しますか？,single,整列させること,AIの目的や価値観を、人間の意図や普遍的な価値観（害を与えない等）に合致（整列）させること。これがズレると、AIが「地球を守るために人類を排除する」といった誤った最適化を行うリスクがある,仲良くすること,並べること,2,AIが能力を持てば持つほど、そのベクトル（目的）が人間と合っているかが人類存亡の鍵になります。,2,5,アライメント,AI安全論,Ethics
Gemini に自分の「悩み」を相談する際、心理カウンセラーの代わりとして使うことの限界は？,single,限界はない,AIは感情を持たず、緊急時の対応（自殺念慮の通報など）もできない。あくまで情報の提供や壁打ち相手であり、専門的な医療行為や危機介入の代替にはならないことを理解しておく,AIの方が優秀,人間は不要,2,「AIに共感された」と感じても、それは計算です。本当の危機には人間の専門家が必要です。,2,5,メンタルヘルス,限界,Ethics
「フィルターバブル」や「エコーチェンバー」を、AIチャットボットが加速させる可能性については？,single,ない,ある。AIが「ユーザーが気に入る回答（同調する回答）」ばかりを生成し続けると、ユーザーの偏った信念が強化され、客観的な事実や反対意見に触れられなくなるリスクがある,AIは中立,ネットの話,2,「心地よい嘘」を言い続けるAIは、人間を孤立した信念の殻に閉じ込めてしまいます。意識的に批判的な意見も求めましょう。,2,5,フィルターバブル,情報摂取,Ethics
企業が「自社専用AIモデル」を作る際、学習データに他社の著作物（新聞記事等）を無断で使うリスクは？,single,バレなければいい,「情報解析」目的を超えて、生成物が元の著作物の市場価値を侵害（競合）する場合、著作権侵害の訴訟リスクがある（NYタイムズ対OpenAI訴訟など）。権利クリアランスの確認が重要,ネットのものは全部フリー,AIのためなら許される,2,「学習は合法」の解釈は国やケースによります。企業活動として行う以上、知財リスクへの配慮は経営課題です。,2,5,著作権,知財管理,Ethics
「レッドチーミング（Red Teaming）」とは、AIのセキュリティ対策で何を行うことですか？,single,赤組になること,敵（攻撃者）の立場になりきって、AIに対して意図的に悪意あるプロンプトを投げかけ、有害な回答や脆弱性を引き出そうとするテストを行い、防御策を強化すること,チームで戦うこと,赤字を減らすこと,2,自分たちで作った盾を、自分たちで矛を持って突く。この泥臭いテストの繰り返しが、AIの安全性を高めます。,2,5,セキュリティ,評価手法,Ethics
これからAIを使う私たち人間に求められる「新しい道徳」とは？,single,AIを崇めること,AIの結果に責任を持つこと、AIを使って他者を傷つけないこと、そしてAIに依存しすぎず人間としての判断力を磨き続けること,AIを壊すこと,何もしないこと,2,テクノロジーが進化しても、それを扱う「人間の品格」が最終的な防波堤です。,2,5,AI倫理,哲学,Ethics
